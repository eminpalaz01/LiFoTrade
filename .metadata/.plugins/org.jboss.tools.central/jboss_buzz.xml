<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Kogito 1.42.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/08/kogito-1-42-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/08/kogito-1-42-0-released.html</id><updated>2023-08-10T23:19:34Z</updated><content type="html">We are glad to announce that the Kogito 1.42.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Fixed an issue that could make live reload not work as expected for workflows that use OpenAPI or gRPC. * Json schema validation: * Changed library from (org,json based)  to (jackson based) * Support nested json schema * Python support * Added support for python method call (embedded python script support was added in previous release) * Process definition addon that allows dynamically uploading SWF files to a Quarkus deployment (Experimental) For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.31.0 artifacts are available at the . A detailed changelog for 1.42.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>The benefits of deploying Ansible Automation Platform on AWS</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/10/benefits-deploying-ansible-automation-platform-aws" /><author><name>Deepankar Jain, Himanshu Yadav</name></author><id>7cad34ec-4b5d-4414-a3d2-639a2c8e6c2f</id><updated>2023-08-10T07:00:00Z</updated><published>2023-08-10T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, you will learn how to use the Red Hat Ansible Automation Platform from the Amazon Web Services (AWS) Marketplace to automatically provision AWS resources.&lt;/p&gt; &lt;h2&gt;Overview of Ansible Automation Platform on AWS &lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; is available on the Amazon Web Services (AWS) Marketplace. It is a self-managed offering that enables enterprise-wide automation with the benefits of deploying &lt;a href="https://www.redhat.com/en/technologies/management/ansible/aws" target="_blank"&gt;Ansible Automation Platform on AWS cloud&lt;/a&gt;. This offering integrates seamlessly with native AWS services and the full Ansible collection for AWS, co-developed and security-tested by AWS and Red Hat.&lt;/p&gt; &lt;p&gt;By combining the power of Red Hat Enterprise Linux, Red Hat OpenShift, and AWS services, developers can effectively scale their cloud infrastructure and leverage technologies such as containers, Kubernetes, and hybrid cloud architecture. The collaboration between Red Hat and AWS offers a hybrid cloud environment that simplifies IT management, reduces complexity, and streamlines innovation.&lt;/p&gt; &lt;h2&gt;4 Components of Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;You can obtain the Ansible Automation Platform offering from &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-l36q6uvlouwb4?sr=0-1&amp;ref_=beagle&amp;applicationId=AWSMPContessa"&gt;AWS Marketplace&lt;/a&gt;. Once you have the subscription, refer to the comprehensive &lt;a href="https://access.redhat.com/documentation/en-us/ansible_on_clouds/2.x/html/red_hat_ansible_automation_platform_from_aws_marketplace_guide/assembly-aap-aws-install"&gt;documentation guide&lt;/a&gt; to set up and configure your Ansible Automation Platform on AWS.&lt;/p&gt; &lt;p&gt;The following is an overview of four components of Ansible Automation Platform.&lt;/p&gt; &lt;h3&gt;1. Execution environment&lt;/h3&gt; &lt;p&gt;Ansible Playbooks run on the execution environment platform. It includes everything needed to run the Ansible Automation Platform, including the runtime environment and dependencies. The Ansible Automation Platform provides a default execution environment that includes many commonly used modules and plugins.&lt;/p&gt; &lt;p&gt;However, you can also create custom execution environments tailored to your specific needs (Figure 1). This allows you to include only the modules and plugins required for your use case, reducing the size of the environment and minimizing security risks. You can manage the execution environment through the Ansible Automation Platform web console or the execution environment builder.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-04-27_11-58-11.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-04-27_11-58-11.png?itok=xXki-AuG" width="600" height="295" alt="A screenshot of the execution environment page of Ansible Automation Platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The execution environment page of Ansible Automation Platform.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To learn more about the execution environment and execution environment builder, refer to the &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/execution_environments.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;2. Inventories&lt;/h3&gt; &lt;p&gt;An inventory is a collection of hosts and groups managed and orchestrated by Ansible Automation Platform. It is used to define and organize the hosts and groups, targeted during an automation job. An inventory can be a static file, a dynamic inventory script, or an inventory plugin.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A static inventory file is a simple text file that lists the hosts and their attributes.&lt;/li&gt; &lt;li&gt;A dynamic inventory script retrieves the inventory information from a third-party system or cloud provider in real time.&lt;/li&gt; &lt;li&gt;Inventory plugins provided for specific platforms, such as Amazon Web Services (AWS) and Microsoft Azure.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In addition to hosts and groups, an inventory can also contain variables that are specific to a host or group. These variables can be used to customize how Ansible interacts with each host or group during a job.&lt;/p&gt; &lt;p&gt;As shown in Figure 2, inventories can be created, imported, and synchronized from external sources, such as cloud providers and configuration management databases (CMDBs). To learn more about Inventories and how to create, manage, and work with them, refer to the official &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/inventories.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-05-22_10-23-30.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-05-22_10-23-30.png?itok=vznwedUp" width="600" height="281" alt="The Inventory page in Ansible Automation Platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Inventory page in Ansible Automation Platform.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;3. Credentials&lt;/h3&gt; &lt;p&gt;In the credentials section of Ansible Automation Platform, you can manage and store sensitive information, such as usernames, passwords, and private keys. These credentials can be used in your playbooks and roles to authenticate with remote hosts, cloud providers, Kubernetes and OpenShift clusters, and other systems.&lt;/p&gt; &lt;p&gt;You can create a variety of credential types, such as SSH, sudo, AWS access keys, tokens, certificates, endpoints, and more. These credentials can be associated with a specific organization, project, or even a specific playbook or role. By using the credential section, you can centrally manage and secure your sensitive data, making it easier to rotate or revoke credentials as needed.&lt;/p&gt; &lt;p&gt;To learn more about using credentials in Ansible Automation Platform, refer to the official &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/credentials.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;4. Projects&lt;/h3&gt; &lt;p&gt;In the projects section of Ansible Automation Platform, you can manage your automation content, including playbooks, roles, collections, modules, and plugins. A project is a collection of related content, such as all the playbooks, roles, and other files that are related to a specific task or application.&lt;/p&gt; &lt;p&gt;Projects can be created and managed through the Ansible Automation Platform web UI or by utilizing the AWX CLI tool. They can be associated with a source control repository, such as Git or SVN, for version control and collaborative development.&lt;/p&gt; &lt;p&gt;Once you have a project set up, you can easily manage and organize your automation content, collaborate with other users, and ensure that your automation runs consistently across your infrastructure. You can also use projects to deploy automation content to remote servers, such as virtual machines or containers, for testing and production use.&lt;/p&gt; &lt;p&gt;For more information on managing projects in Ansible Automation Platform, you can refer to the &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/projects.html"&gt;user guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Job and workflow templates &lt;/h2&gt; &lt;p&gt;Job templates and workflow templates are powerful features of the Ansible Automation Platform that enable IT teams to automate and orchestrate their infrastructure management tasks.&lt;/p&gt; &lt;p&gt;A job template is a predefined set of tasks that can be executed on one or more hosts, making it a powerful tool for automating repetitive tasks, such as software installations or configuration changes. With job templates, we can define the tasks to be executed, the hosts on which they should run, and any required input parameters, all through an intuitive user interface.&lt;/p&gt; &lt;p&gt;On the other hand, workflow templates allow you to chain together multiple job templates to create a more complex process or workflow. With workflow templates, you can automate even the most complex tasks, such as deploying a multi-tiered application with multiple components and dependencies.&lt;/p&gt; &lt;p&gt;Together, job and workflow templates provide a comprehensive automation solution that streamlines IT operations and ensures that your infrastructure is configured and maintained consistently and efficiently.&lt;/p&gt; &lt;h2&gt;Find more resources&lt;/h2&gt; &lt;p&gt;To learn about using&lt;strong&gt; &lt;/strong&gt;job templates to create Amazon Web Services (AWS) EC2 instances with Ansible Automation Platform, check out our step-by-step guide for &lt;a href="https://developers.redhat.com/articles/2023/04/28/step-step-guide-creating-amazon-web-services-aws-ec2-instance-using-ansible"&gt;How to create an EC2 instance in AWS using Ansible automation&lt;/a&gt;. For information on creating instances using workflow templates, read the article, &lt;a href="https://developers.redhat.com/articles/2023/04/28/step-step-guide-creating-amazon-web-services-aws-ec2-instance-workflow-using"&gt;How to create an EC2 instance in AWS using Ansible workflow&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want understand automation more in-depth, refer to the &lt;a href="https://developers.redhat.com/e-books/it-executives-guide-automation"&gt;IT Executive's Guide to Automation&lt;/a&gt; e-book, which provides a comprehensive overview of automation and its impact on businesses. If you're new to Ansible Automation Platform, you can &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download&lt;/a&gt; it and &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;get started by exploring interactive labs&lt;/a&gt; at no cost.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/10/benefits-deploying-ansible-automation-platform-aws" title="The benefits of deploying Ansible Automation Platform on AWS"&gt;The benefits of deploying Ansible Automation Platform on AWS&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Deepankar Jain, Himanshu Yadav</dc:creator><dc:date>2023-08-10T07:00:00Z</dc:date></entry><entry><title>The process of migrating Java applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/09/process-migrating-java-applications" /><author><name>Mudassar Iqbal</name></author><id>57e90918-1b0b-45f8-9acb-f93d4d1aa19e</id><updated>2023-08-09T07:00:00Z</updated><published>2023-08-09T07:00:00Z</published><summary type="html">&lt;p&gt;The migration process is cumbersome, to say the least. The root cause for this can be associated with the ambiguity about the differences that may exist between two vantage points (i.e., origin and destination). Hence in the absence of a knowledgeable, tried, and tested roadmap, one simply cannot identify the gaps that are to be bridged while moving from origin to the destination. Same goes for software migration. A software migration process must factor in the changes it expects to undergo while migrating (i.e., downgrading, upgrading, or switching between two different and/or compatible/incompatible versions of the same or different brands). The key to success for such a venture lies in knowing the unknowns alongside their respective impact to its surroundings. Only then can an effective list of changes be identified and implemented.&lt;/p&gt; &lt;p&gt;Recently, I had the opportunity to work on the same lines where few enterprises were looking to move away from an incumbent Java Development Kit (JDK) and Java Runtime Environment (JRE) in favor of the Red Hat build of OpenJDK. JDK and JRE are the most critical components of a Java based IT landscape and can inflict widespread impact to its dependents. While the adapted process during these engagements was specific to Red Hat build of OpenJDK, migration between other JDKs and softwares can also draw inspiration from it.&lt;/p&gt; &lt;h2&gt;Inventory list&lt;/h2&gt; &lt;p&gt;Like any migration process, the process to migrate to Red Hat build of OpenJDK from any other JDK involves detailed assessment and planning to ensure that all unknowns responsible for causing impact and complexity are identified and dealt with carefully. The assessment process should always start by listing the inventory.&lt;/p&gt; &lt;p&gt;Before migrating, we need to identify every single component that may face an impact, including computers, servers (bare metal and/or virtual machines), workstations (physical or virtual desktops and/or laptops), Java processes (application servers, binary executables, CI/CD pipelines and more importantly the business applications). In terms of business applications, a distinction must be made between the ones developed and maintained in-house or procured from a third-party vendor. The reason for this will be discussed later under Application Migration. Long story short, anything that is impacted by a change in Java version or vendor must be listed as an item to the inventory.&lt;/p&gt; &lt;h2&gt;Artifact screening&lt;/h2&gt; &lt;p&gt;Human errors are probably the most common factor contributing to a failure. Although inventory is usually documented by someone who knows the whole infrastructure well, still there is a possibility to miss something. This possibility appears more probable for some less frequently utilized or upgraded Java based artifact. To eliminate this, simple automation can be effective. For example, a simple automation process can be developed that scans every single host to locate files of type Jar, War, or Ear. This can be very easily achieved with the help of a commonly available scripting technology like bash, PowerShell or Red Hat Ansible Automation Platform, etc.&lt;/p&gt; &lt;h2&gt;Automate Java installation&lt;/h2&gt; &lt;p&gt;One of the most critical tasks during JDK/JRE migration is the installation of the binaries for the intended Java version released by the preferred vendor. However, this process is extremely laborious. What makes it complex is the varsity and size associated with the targeted infrastructure. For example, even the infrastructure maintained by small to medium sized enterprises is a handful, let alone the larger enterprises. The reasons for this are multifold. Hardware infrastructure consists of a large number of computers that may run different types of operating systems (i.e., servers usually operate with Linux or Windows whereas workstations can run macOS or Windows).&lt;/p&gt; &lt;p&gt;Additionally, Java is made available for installation in different ways. Some organizations allow direct download, some make it available for installation via an internal marketplace, and some organizations ensure that it is installed while provisioning the workstation. On top of this, different vendors release different packaging like RPM, binary archive, or even a graphical installer.&lt;/p&gt; &lt;p&gt;All these multidimensional facts suggest that each installation task must be handled uniquely. As each host is expected to be migrated to a newer Java brand and/or version individually, the installation process must be capable of successfully repeating itself without failing. Hence, automation using a tool like Ansible Automation Platform is the key to success.&lt;/p&gt; &lt;h2&gt;Application migration&lt;/h2&gt; &lt;p&gt;Probably the most critical task of JDK/JRE migration is to make in-house and third-party business applications compatible with newer versions and/or brands of Java. As JDK/JRE provide compile time as well as runtime support to these applications, these components are expected to endure the majority of the impact. One can draw comfort from a simple rule, cross vendor migration between competing versions may inflict minimal to no impact at all. However with the wider gaps between two versions, the impact is also expected to be greater considering that implementation may have changed and the libraries, classes, methods, etc. may have been deprecated and/or replaced. Yet we need a better approach to handle this task.&lt;/p&gt; &lt;p&gt;Third-party applications cannot be upgraded or compiled in-house. This is why the vendor(s) should be providing clarity around these applications if they are tested and supported by them with the Java version/brand intended to provide the runtime and/or compile time support. In case there is a need for some patch work or upgrade, it should also be handled by the respective vendor(s).&lt;/p&gt; &lt;p&gt;Contrary to third-party applications, in-house applications are designed, developed, and maintained by the enterprise. These applications must undergo an assessment for the expected impact. Windup or migration toolkit for applications are good tools for this purpose. This will identify the changes needed for successful compilation of the source code to operate with the intended JRE.&lt;/p&gt; &lt;h2&gt;Delivery and verification&lt;/h2&gt; &lt;p&gt;Now that we have the infrastructure migrated to operate with a newer or different version or brand of Java, development environment alongside CI/CD pipelines are migrated to use intended Java version or brand during development and compilation time. Business applications are refactored alongside unit tests. It is still strongly recommended to verify everything for functional correctness. Once the business applications have passed the verification phase, migration can be deemed successful.&lt;/p&gt; &lt;h2&gt;Navigating the software migration process&lt;/h2&gt; &lt;p&gt;Even though software migration involves uncertainty, there are methods and tools available to deal with it. What matters is how effectively you utilize these methodologies and technologies.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/09/process-migrating-java-applications" title="The process of migrating Java applications"&gt;The process of migrating Java applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mudassar Iqbal</dc:creator><dc:date>2023-08-09T07:00:00Z</dc:date></entry><entry><title type="html">DMN boxed expression editor improvements</title><link rel="alternate" href="https://blog.kie.org/2023/08/dmn-boxed-expression-editor-improvements.html" /><author><name>Jozef Marko</name></author><id>https://blog.kie.org/2023/08/dmn-boxed-expression-editor-improvements.html</id><updated>2023-08-08T14:11:16Z</updated><content type="html">We are happy to announce another set of improvements to our boxed expression editor. As we unveiled recently in the , we try to improve the boxed expression editor continuously. Described features from this article will be available with release. So let’s have a look at them in more detail. AUTOCOMPLETION Autocompletion appears automatically as the user starts to type the text of the expression. These autocompletion are filtered to match the typed text. As an alternative, the user can invoke autocompletion simply by pressing Ctrl + Spacebar (the shortcut may differ on various platforms). BEFORE AND NOW Older versions of the DMN boxed expression editor suggested a raw list of available FEEL functions to the user. We can understand raw as missing any documentation of the function. However now, thanks to , we added documentation for each available function containing details of parameters and returned value. ROWS AND COLUMNS INSERTION Row or column insertion is an operation a DMN modeler user executes daily. It can be done using an inline plus icon or context menu, that is invoked by a right mouse button click, from most of the boxed expression editor cells. BEFORE AND NOW Older versions of the DMN boxed expression editor allowed single row or column insertion per time. Creation of large expressions this way is slow, not efficient enough. However now, thanks to , users are able to insert as many columns or rows per time as they want. Well, we actually set an upper boundary of 500 items, we believe this boundary is sufficient. These new boxed expression editor improvements will be available soon in the next release. If you spot any issue feel free to contact us on or report an on GitHub. The post appeared first on .</content><dc:creator>Jozef Marko</dc:creator></entry><entry><title>How to monitor workloads using OpenShift monitoring stack</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/08/how-monitor-workloads-using-openshift-monitoring-stack" /><author><name>Jude Niroshan</name></author><id>cb65c017-d049-40bf-8af6-eda4b988872d</id><updated>2023-08-08T07:00:00Z</updated><published>2023-08-08T07:00:00Z</published><summary type="html">&lt;p&gt;The out-of-the-box monitoring stack in Red Hat OpenShift provides a comprehensive set of tools and services to monitor various aspects of your workloads, including metrics, logs, events, and traces. With this monitoring stack, you can gain valuable insights into the health and performance of your applications, identify and troubleshoot issues, and optimize resource utilization. This allows you to deliver a seamless user experience and confidently meet your business objectives.&lt;/p&gt; &lt;p&gt;In this article, we will explore the monitoring stack in Red Hat OpenShift and how to use it to monitor a sample application effectively using &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt; and &lt;a href="https://grafana.com/oss/grafana"&gt;Grafana&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;How to monitor a sample application&lt;/h2&gt; &lt;p&gt;We will use an OpenShift cluster with version 4.10.36 in this example.&lt;/p&gt; &lt;h3&gt;Step 1: Create a ConfigMap&lt;/h3&gt; &lt;p&gt;Create a ConfigMap in the openshift-monitoring namespace.&lt;/p&gt; &lt;p&gt;In OpenShift, user workload refers to the applications and services that users deploy onto the cluster. Before we begin, we need to create a ConfigMap in the openshift-monitoring namespace to enable user workload monitoring.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: | enableUserWorkload: true&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 2: Check for Prometheus&lt;/h3&gt; &lt;p&gt;Check openshift-user-workload-monitoring namespace for the new Prometheus server pod (Figure 1).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_article_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_article_1.png?itok=pneaClZJ" width="600" height="175" alt="A screenshot of the Prometheus server pod in OpenShift Container Platform.." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The Prometheus server pod is located in the Pod page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After creating the ConfigMap, check the openshift-user-workload-monitoring namespace for a new Prometheus server pod.&lt;/p&gt; &lt;h3&gt;Step 3: Deploy Java application&lt;/h3&gt; &lt;p&gt;Deploy the example Java application as a deployment with an OCP route.&lt;/p&gt; &lt;p&gt;A &lt;a href="https://github.com/JudeNiroshan/coin-toss-api"&gt;sample Java applicatio&lt;/a&gt;n has been developed that provides an API for coin-tossing. This API exposes the number of times the heads side of the coin showed, represented as a coin_heads_total metric of counter type in Prometheus. The &lt;a href="https://quarkus.io/guides/micrometer"&gt;MicroMeter Quarkus extension&lt;/a&gt; has been utilized to expose this metric. The metrics can be accessed via the http://localhost:8080/q/metrics endpoint if the application starts locally. The application has been containerized and is available to use under &lt;strong&gt;quay.io/jnirosha/coin-toss-api&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Next, deploy the coin-toss-api as a deployment with an OpenShift route. Use the following command to deploy the application:&lt;/p&gt; &lt;p&gt;&lt;code&gt;oc new-app --name=coin-toss-api quay.io/jnirosha/coin-toss-api&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;oc expose svc/coin-toss-api&lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Step 4: Create a ServiceMonitor instance&lt;/h3&gt; &lt;p&gt;Create a ServiceMonitor instance to register the Prometheus target.&lt;/p&gt; &lt;p&gt;To monitor the coin-toss-api application, we must create a ServiceMonitor instance to register the Prometheus target. Run the following command to create the ServiceMonitor:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata:   name: my-service-monitor   namespace: my-project spec:   endpoints:     - interval: 5s       path: /q/metrics       port: 8080-tcp       scheme: http   namespaceSelector:     matchNames:       - my-project   selector:     matchLabels:       app: coin-toss-api&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 5: Filter targets&lt;/h3&gt; &lt;p&gt;After creating the ServiceMonitor, navigate to the Prometheus UI by clicking on the &lt;strong&gt;Observe&lt;/strong&gt; tab in the OpenShift console. Under the &lt;strong&gt;Targets&lt;/strong&gt; menu, filter the targets by the user (Figure 2).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_article_2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_article_2.png?itok=tCSNn6Vc" width="600" height="295" alt="A screenshot of the filter tool under the targets tab." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Click on the Observe and Targets tab to filter metrics targets.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 6: Observe metrics&lt;/h3&gt; &lt;p&gt;Navigate to the &lt;strong&gt;Observe&lt;/strong&gt; tab in the OpenShift console, then select &lt;strong&gt;Metrics&lt;/strong&gt; options (Figure 3).&lt;/p&gt; &lt;p&gt;In the expressions input field, you can type &lt;code&gt;coin_heads_total&lt;/code&gt;, which is our custom metric name.&lt;/p&gt; &lt;p&gt;Then click the &lt;strong&gt;Run queries&lt;/strong&gt; button.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_article_3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_article_3.png?itok=-HZlQ3PO" width="600" height="303" alt="Custom metric in Openshift console" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Run Query&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;At this point, we have two different Prometheus server instances. One is for default OpenShift monitoring, and the other is for user workload monitoring. In OpenShift, to observe metrics, we can see metrics scraped through both the Prometheus server instances (Figure 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/diagram.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/diagram.jpeg?itok=__RrRn5G" width="600" height="496" alt="A diagram showing the two Prometheus server instances." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The two Prometheus server instances for workload monitoring.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 7: Install the Grafana operator&lt;/h3&gt; &lt;p&gt;Install the Grafana operator in a different namespace. To install the Grafana operator, we recommend creating a new namespace. Run the following command to create the namespace as follows:&lt;/p&gt; &lt;p&gt;&lt;code&gt;oc create namespace my-grafana&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then, install the Grafana operator using OperatorHub.&lt;/p&gt; &lt;h3&gt;Step 8: Create a Grafana instance&lt;/h3&gt; &lt;p&gt;Create a Grafana instance from the operator.&lt;/p&gt; &lt;p&gt;After installing the Grafana operator (v4.9.0), create a Grafana instance by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: integreatly.org/v1alpha1 kind: Grafana metadata:   name: my-grafana   namespace: my-grafana spec:   config:     security:       admin_user: admin       admin_password: my-password dataStorage: accessModes: - ReadWriteOnce class: gp2 size: 1Gi   ingress:     enabled: true     tls:       enabled: true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Configure Grafana to use an ingress with TLS enabled and setting an admin username and password. Make sure to replace "my-password" with a strong password.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note: &lt;/strong&gt;Please ensure that you include the available storage class in your cluster within the &lt;code&gt;dataStorage&lt;/code&gt; section.&lt;/p&gt; &lt;h3&gt;Step 9: Create a cluster role&lt;/h3&gt; &lt;p&gt;Create a cluster role binding to the grafana-serviceaccount.&lt;/p&gt; &lt;p&gt;To allow Grafana to access monitoring data in the OpenShift cluster, we need to create a cluster role binding to the grafana-serviceaccount.&lt;/p&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;oc create clusterrolebinding grafana-view --clusterrole=cluster-monitoring-view --serviceaccount=my-grafana:grafana-serviceaccount&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This command creates a cluster role binding named grafana-view that provides the cluster-monitoring-view role to the grafana-serviceaccount in all namespaces.&lt;/p&gt; &lt;h3&gt;Step 10: Generate a token&lt;/h3&gt; &lt;p&gt;Generate a token using the grafana-serviceaccount.&lt;/p&gt; &lt;p&gt;To authenticate with the OpenShift API, Grafana needs a token generated from the grafana-serviceaccount. Run the following command to generate a token:&lt;/p&gt; &lt;p&gt;&lt;code&gt;export TOKEN=$(oc create token grafana-serviceaccount -n my-grafana) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then view the token using:&lt;/p&gt; &lt;p&gt;&lt;code&gt;echo $TOKEN&lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Step 11: Create a GrafanaDataSource instance&lt;/h3&gt; &lt;p&gt;Create a GrafanaDataSource instance from the operator.&lt;/p&gt; &lt;p&gt;Finally, create a GrafanaDataSource instance from the operator to connect Grafana to Prometheus. Run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata:   name: prometheus   namespace: my-grafana spec:   datasources:     - basicAuthUser: internal       access: proxy       editable: true       secureJsonData:         httpHeaderValue1: &gt;-           Bearer &lt;&lt;paste-your-generated-token-here&gt;&gt;       name: Prometheus       url: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091'       jsonData:         httpHeaderName1: Authorization         timeInterval: 5s         tlsSkipVerify: true       basicAuth: false       isDefault: true       version: 1       type: prometheus   name: test_name&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Log in to the Grafana dashboard and check data sources. You should see an entry like Figure 5. This is the data source we created using the custom resource in the operator.&lt;/p&gt; &lt;p&gt;Ensure that the Grafana can extract metrics from the Prometheus servers.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_article_5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_article_5.png?itok=87MRjNry" width="600" height="548" alt="A screenshot of the Grafana dashboard." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: The data source in the Grafana dashboard is displayed.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once everything is working as expected, you can create your own Grafana dashboard. Figure 6 shows a simple graph I created inside the Grafana. It shows how many times the heads have appeared so far when tossing the coin in our Java application.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-03-13_at_11.42.14.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-03-13_at_11.42.14.png?itok=R1Olh8dT" width="600" height="282" alt="A Grafana graph created to show the number of coins tossed." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: Grafana graph&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;The OpenShift monitoring stack is easy to use&lt;/h2&gt; &lt;p&gt;In conclusion, using a monitoring stack on the Red Hat OpenShift platform is easy and straightforward. One of the major advantages of this platform is that it comes out of the box with all the essential components necessary to enable effective monitoring of your applications and infrastructure. With just a few configuration settings, you can easily set up monitoring tools like Prometheus, Grafana, and alert manager to provide insights into the performance and health of your applications.&lt;/p&gt; &lt;p&gt;Overall, the OpenShift platform provides a powerful and easy-to-use monitoring stack that simplifies the process of monitoring complex applications and infrastructure. With the ability to customize and extend the platform's monitoring capabilities, you can easily create a monitoring solution that meets the specific needs of your organization.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/08/how-monitor-workloads-using-openshift-monitoring-stack" title="How to monitor workloads using OpenShift monitoring stack"&gt;How to monitor workloads using OpenShift monitoring stack&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jude Niroshan</dc:creator><dc:date>2023-08-08T07:00:00Z</dc:date></entry><entry><title>Back up Kubernetes persistent volumes using OADP</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/07/back-kubernetes-persistent-volumes-using-oadp" /><author><name>Jorge Balderas</name></author><id>12071637-6791-4339-9caf-dbd9ee480991</id><updated>2023-08-07T07:00:00Z</updated><published>2023-08-07T07:00:00Z</published><summary type="html">&lt;p&gt;OpenShift APIs for Data Protection (OADP) is an operator that lets you back up and restore workloads in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; clusters. It is based on the upstream open source project &lt;a href="https://velero.io" target="_blank"&gt;Velero&lt;/a&gt;. You can use OADP to backup and restore all &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; resources for a given project, including persistent volumes.&lt;/p&gt; &lt;p&gt;It is a best practice to be able to recreate your workloads via Infrastructure as Code (IAC) pipelines or &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt;. Most Kubernetes projects in production already have a way to be recreated; however, when it comes to restoring data from persistent volumes, that requires a separate solution. OADP can fill that gap. This article will focus primarily on restoring persistent volumes.&lt;/p&gt; &lt;h3&gt;How does OADP back up persistent volumes?&lt;/h3&gt; &lt;p&gt;OADP allows backing up and restoring persistent volumes via either &lt;a href="https://restic.net" target="_blank"&gt;Restic&lt;/a&gt; or &lt;a href="https://access.redhat.com/articles/5456281#does-oadp-support-csi-snapshots-12" target="_blank"&gt;CSI snapshots&lt;/a&gt;. In both cases, incremental backups are supported.&lt;/p&gt; &lt;h3&gt;What are some limitations of backing up data with OADP?&lt;/h3&gt; &lt;p&gt;These are some key limitations that are worth highlighting:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Pods need to be running for the corresponding persistent volumes to be backed up.&lt;/li&gt; &lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir" target="_blank"&gt;Emptydir volumes&lt;/a&gt; cannot be backed up. Your workloads should not be storing important data in &lt;code&gt;emptydir&lt;/code&gt; volumes, as these volumes are ephemeral.&lt;/li&gt; &lt;li&gt;Persistent volumes cannot exist when doing a restore. This means that the corresponding persistent volume claims will need to be deleted explicitly before doing a restore.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Install and configure OADP&lt;/h2&gt; &lt;p&gt;Follow these steps to install and configure the OpenShift APIs for Data Protection Operator.&lt;/p&gt; &lt;h3&gt;Prerequisites&lt;/h3&gt; &lt;ul&gt;&lt;li&gt;OpenShift cluster configured (using 4.12 for this demo)&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.12/cli_reference/openshift_cli/getting-started-cli.html" target="_blank"&gt;oc (OpenShift client) CLI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Amazon Web Services (AWS) account&lt;/li&gt; &lt;li&gt;&lt;a href="https://aws.amazon.com/cli/" target="_blank"&gt;AWS CLI&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Install the OADP Operator&lt;/h3&gt; &lt;p&gt;For this demo, we used Red Hat OpenShift 4.12 with OADP Operator version 1.1.3, which uses Velero version 1.9.5. To install via OpenShift Console follow these &lt;a href="https://docs.openshift.com/container-platform/4.12/backup_and_restore/application_backup_and_restore/installing/installing-oadp-aws.html#oadp-installing-operator_installing-oadp-aws" target="_blank"&gt;simple steps&lt;/a&gt; (requires user with cluster-admin role):&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;In the OpenShift Container Platform web console, click &lt;strong&gt;Operators&lt;/strong&gt; → &lt;strong&gt;OperatorHub&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Use the &lt;strong&gt;Filter by keyword&lt;/strong&gt; field to find the &lt;strong&gt;OADP Operator&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Select the &lt;strong&gt;OADP Operator&lt;/strong&gt; (select the one from Red Hat source, instead of the Community source) and click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Accept default values and click &lt;strong&gt;Install&lt;/strong&gt; to install the Operator in the &lt;code&gt;openshift-adp&lt;/code&gt; project.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click &lt;strong&gt;Operators&lt;/strong&gt; → &lt;strong&gt;Installed Operators&lt;/strong&gt; to verify the installation.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Create an S3 bucket for backups&lt;/h3&gt; &lt;p&gt;For this demo we used an AWS S3 bucket; however, any S3 compliant storage can be used, including &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/managing_hybrid_and_multicloud_resources/object-bucket-claim" target="_blank"&gt;ODF (OpenShift Data Foundation) object bucket claims&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The following instructions are based on the &lt;a href="https://docs.openshift.com/container-platform/4.12/backup_and_restore/application_backup_and_restore/installing/installing-oadp-aws.html#migration-configuring-aws-s3_installing-oadp-aws" target="_blank"&gt;OpenShift documentation&lt;/a&gt;:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Log into AWS using &lt;code&gt;aws configure&lt;/code&gt; and provide your credentials.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Set the &lt;code&gt;BUCKET&lt;/code&gt; and &lt;code&gt;REGION&lt;/code&gt; variables:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="terminal"&gt;BUCKET=&lt;your_bucket&gt; REGION=&lt;your_region&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create an AWS S3 bucket (&lt;code&gt;us-east-1&lt;/code&gt; does not support a &lt;code&gt;LocationConstraint&lt;/code&gt;. If your region is &lt;code&gt;us-east-1&lt;/code&gt;, omit &lt;code&gt;--create-bucket-configuration LocationConstraint=$REGION&lt;/code&gt;&lt;span&gt;):&lt;/span&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="terminal"&gt;aws s3api create-bucket --bucket $BUCKET --region $REGION \ --create-bucket-configuration LocationConstraint=$REGION &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create an IAM user (If you want to use Velero to back up multiple clusters with multiple S3 buckets, create a unique user name for each cluster&lt;span&gt;):&lt;/span&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="terminal"&gt;aws iam create-user --user-name velero &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;velero-policy.json&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="terminal"&gt;cat &gt; velero-policy.json &lt;&lt;EOF { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "ec2:DescribeVolumes", "ec2:DescribeSnapshots", "ec2:CreateTags", "ec2:CreateVolume", "ec2:CreateSnapshot", "ec2:DeleteSnapshot" ], "Resource": "*" }, { "Effect": "Allow", "Action": [ "s3:GetObject", "s3:DeleteObject", "s3:PutObject", "s3:AbortMultipartUpload", "s3:ListMultipartUploadParts" ], "Resource": [ "arn:aws:s3:::${BUCKET}/*" ] }, { "Effect": "Allow", "Action": [ "s3:ListBucket", "s3:GetBucketLocation", "s3:ListBucketMultipartUploads" ], "Resource": [ "arn:aws:s3:::${BUCKET}" ] } ] } EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Attach the policies to give the &lt;code&gt;velero&lt;/code&gt; user the minimum necessary permissions:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="terminal"&gt;aws iam put-user-policy --user-name velero --policy-name velero \ --policy-document file://velero-policy.json&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create an access key for the &lt;code&gt;velero&lt;/code&gt; user:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="terminal"&gt;aws iam create-access-key --user-name velero&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;credentials-velero&lt;/code&gt; file replacing the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the previous command output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="terminal"&gt;cat &lt;&lt; EOF &gt; ./credentials-velero [default] aws_access_key_id=$AWS_ACCESS_KEY_ID aws_secret_access_key=$AWS_SECRET_ACCESS_KEY EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Configure OADP to use S3 for backups&lt;/h2&gt; &lt;p&gt;This sections creates the &lt;code&gt;DataProtectionApplication&lt;/code&gt; that will be used to schedule backups.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Log in to OpenShift using &lt;code&gt;oc cli login&lt;/code&gt; command.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;Secret&lt;/code&gt; object with the &lt;code&gt;credentials-velero&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="terminal"&gt;oc create secret generic cloud-credentials -n openshift-adp \ --from-file cloud=credentials-velero &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Create the &lt;code&gt;DataProtectionApplication&lt;/code&gt; file: &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt; EOF &gt; ./dpa.yaml apiVersion: oadp.openshift.io/v1alpha1 kind: DataProtectionApplication metadata: name: dpa namespace: openshift-adp spec: configuration: restic: enable: true velero: defaultPlugins: - aws backupLocations: - velero: config: region: ${REGION} profile: default credential: key: cloud name: cloud-credentials objectStorage: bucket: ${BUCKET} prefix: demo default: true provider: aws EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Create &lt;strong&gt;DataProtectionApplication&lt;/strong&gt; by running this command: &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f dpa.yaml -n openshift-adp&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Confirm the application is ready for backup by confirming the status of the &lt;code&gt;BackupStorageLocation&lt;/code&gt; is available: &lt;pre&gt; &lt;code class="language-bash"&gt;oc get BackupStorageLocation -n openshift-adp NAME PHASE LAST VALIDATED AGE DEFAULT dpa-1 Available 58s 81m true&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Deploy sample application: WordPress&lt;/h2&gt; &lt;p&gt;Next, we need to deploy a Kubernetes application that uses persistent volumes. We will use WordPress, which uses the MySQL database. The instructions were based on this &lt;a href="https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/" target="_blank"&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Download the &lt;code&gt;MySQL&lt;/code&gt; deployment configuration file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="shell"&gt;curl -LO \ https://raw.githubusercontent.com/yortch/oadp/main/mysql-deployment.yaml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Download the WordPress configuration file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="shell"&gt;curl -LO \ https://raw.githubusercontent.com/yortch/oadp/main/wordpress-deployment.yaml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Export &lt;code&gt;PASSWORD&lt;/code&gt; as an environment variable:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;PASSWORD=&lt;YOUR_PASSWORD&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Generate the following &lt;code&gt;kustomization.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash" data-lang="shell"&gt;cat &lt;&lt;EOF &gt;./kustomization.yaml secretGenerator: - name: mysql-pass literals: - password=${PASSWORD} resources: - mysql-deployment.yaml - wordpress-deployment.yaml EOF &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Create a new application project: &lt;pre&gt; &lt;code class="language-bash"&gt;oc new-project wordpress&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Apply the &lt;code&gt;kustomization.yaml&lt;/code&gt; file: &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -n wordpress -k ./&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Next, expose the WordPress service: &lt;pre&gt; &lt;code class="language-bash"&gt;oc expose service wordpress -n wordpress&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Print the service URL and navigate to it from a browser. &lt;pre&gt; &lt;code class="language-bash"&gt;echo http://$(oc get route -n wordpress -o jsonpath='{.items[0].spec.host}')&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Proceed with the initial WordPress setup so that it is included in the backup.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Back up the WordPress application&lt;/h2&gt; &lt;ol&gt;&lt;li&gt;Create a &lt;code&gt;Backup&lt;/code&gt; resource file (the default timeout is 30 days/720 hours, which is how long the data will be retained in the backup): &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOF &gt;./backup.yaml apiVersion: velero.io/v1 kind: Backup metadata: name: backup namespace: openshift-adp spec: includedNamespaces: - wordpress defaultVolumesToRestic: true ttl: 720h EOF&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Apply the &lt;code&gt;backup.yaml&lt;/code&gt; file &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -n openshift-adp -f backup.yaml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Confirm that the pod volumes backup status show status as &lt;code&gt;Completed&lt;/code&gt;: &lt;pre&gt; &lt;code class="language-bash"&gt;oc get PodVolumeBackup -n openshift-adp&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Simulate disaster event&lt;/h2&gt; &lt;p&gt;Before we can restore from the backup, we need to simulate a disaster event by deleting the deployments and corresponding persistent volume claims and persistent volumes. Remember that the goal is to simulate the persistent volumes are lost so that they can be restored successfully.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc delete Deployment,PersistentVolumeClaim,PersistentVolume --all -n wordpress&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Navigate to the WordPress application URL to confirm that the application is no longer functional.&lt;/p&gt; &lt;h2&gt;Restore WordPress application from backup&lt;/h2&gt; &lt;p&gt;Next, proceed and create a &lt;code&gt;Restore&lt;/code&gt; resource file specifying the backup name used earlier:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOF &gt;./restore.yaml apiVersion: velero.io/v1 kind: Restore metadata: name: restore namespace: openshift-adp spec: backupName: backup EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Apply the restore file so that the restore is triggered:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -n openshift-adp -f restore.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Monitor progress of the pod volume restore until the status changes to Completed:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get PodVolumeRestore -n openshift-adp&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Navigate back to the WordPress application URL and confirm that it shows the demo site instead of the initial WordPress setup screen. This proves that the persistent volumes were restored to the version that was backed up.&lt;/p&gt; &lt;h2&gt;Troubleshooting and other tips&lt;/h2&gt; &lt;p&gt;This section covers some additional tips to keep in mind.&lt;/p&gt; &lt;h3&gt;Viewing backup/restore logs&lt;/h3&gt; &lt;p&gt;Backup and restore logs can be viewed from the &lt;code&gt;openshift-adp&lt;/code&gt; namespace pod logs. However, because multiple pods are running and you could be running multiple backups, it is often challenging to find the corresponding backup logs. Alternatively, logs can be downloaded from the S3 bucket using &lt;a href="https://www.filestash.app/aws-s3-cli.html" target="_blank"&gt;S3 CLI commands&lt;/a&gt;. As an example, this command will download restore logs for a backup named &lt;code&gt;backup&lt;/code&gt;, which uses the &lt;code&gt;demo&lt;/code&gt; prefix:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;aws s3 mv s3://oadp/demo/restores/restore/restore-restore-results.gz .&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Another helpful command is to list files recursively. For instance, the command below will list all files for a bucket named &lt;code&gt;oadp&lt;/code&gt;, which uses the &lt;code&gt;demo&lt;/code&gt; prefix:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;aws s3 ls --recursive s3://oadp/demo/&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Increasing backup/restore timeout&lt;/h3&gt; &lt;p&gt;For backups with hundreds of gigabytes, it can take over an hour (which is the default timeout) to initial backup completion (subsequent incremental backups will take less time). To change the timeout for &lt;code&gt;Restic&lt;/code&gt; volume backups, you can set it via &lt;a href="https://docs.openshift.com/container-platform/4.12/backup_and_restore/application_backup_and_restore/oadp-api.html#oadp-api-tables" target="_blank"&gt;&lt;code&gt;spec.configuration.restic.timeout&lt;/code&gt;&lt;/a&gt; in the &lt;code&gt;DataProtectionApplication&lt;/code&gt; instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;spec: configuration: restic: enable: true timeout: 2h&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Scheduling backups&lt;/h3&gt; &lt;p&gt;Velero provides a convenient resource that can be used to schedule backups: the &lt;a href="https://docs.openshift.com/container-platform/4.12/backup_and_restore/application_backup_and_restore/backing_up_and_restoring/backing-up-applications.html#oadp-scheduling-backups_backing-up-applications" target="_blank"&gt;&lt;code&gt;Schedule&lt;/code&gt;&lt;/a&gt; resource. The &lt;code&gt;Schedule&lt;/code&gt; resource has essentially the same definition as a &lt;code&gt;Backup&lt;/code&gt; resource, but it has an additional &lt;code&gt;schedule&lt;/code&gt; property that is used to specify a &lt;code&gt;cron&lt;/code&gt; expression to specify when and how frequent to take backups. Here is a sample &lt;code&gt;Schedule&lt;/code&gt; definition, which will create a daily backup at 7 a.m.:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;apiVersion: velero.io/v1 kind: Schedule metadata: name: &lt;schedule&gt; namespace: openshift-adp spec: schedule: 0 7 * * * template: includedNamespaces: - &lt;namespace&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;In this article, we learned how you can use OADP to easily back up OpenShift applications, including persistent volumes.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/07/back-kubernetes-persistent-volumes-using-oadp" title="Back up Kubernetes persistent volumes using OADP"&gt;Back up Kubernetes persistent volumes using OADP&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jorge Balderas</dc:creator><dc:date>2023-08-07T07:00:00Z</dc:date></entry><entry><title type="html">PrimeFaces with Jakarta EE 10 made simple</title><link rel="alternate" href="https://www.mastertheboss.com/web/primefaces/primefaces-with-jakarta-ee-10-made-simple/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/web/primefaces/primefaces-with-jakarta-ee-10-made-simple/</id><updated>2023-08-04T17:54:53Z</updated><content type="html">In this article we will discuss how to create and deploy a Jakarta EE 10 application that uses Primefaces as Jakarta Faces implementation. We will build a sample application that will run in microservice-style as WildFly Bootable JAR. PrimeFaces extends Jakarta Faces by providing a wide range of rich and customizable UI components that are ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Tips for upgrading H2 database in your test-suite</title><link rel="alternate" href="https://blog.kie.org/2023/08/tip-h2-upgrade.html" /><author><name>Gonzalo Muñoz Fernández</name></author><id>https://blog.kie.org/2023/08/tip-h2-upgrade.html</id><updated>2023-08-03T09:04:12Z</updated><content type="html">“Companies pay too much attention to the cost of doing something. They should worry more about the cost of not doing it.” (Philip Kotler) In the world of IT, there’s an old adage that states: "Months of debugging can save days of reading the documentation." Jokes aside, while documentation is crucial for understanding software libraries, it may not always provide all the answers, especially when upgrading a library to newer versions. Unexpected issues and regressions can arise, requiring additional help from the developer community. In this brief post, we aim to give some tips for H2 upgrade (based on ) with the purpose of assisting other developers facing challenges when upgrading the H2 library from an early version (e.g., 1.4.19x) to a newer release (e.g., 2.2.220, as in our case).  The H2 in-memory database is widely used for persistence tests in KIE projects and hadn’t been updated to recent versions for years. As a result, we foresaw that there would be a significant challenge. “Between version 1.4.200 and version 2.0.202 there have been considerable changes, such that a simple update is not possible” , and we were even before that point. So hands-on, and let’s dive in and explore some tips for H2 upgrade with samples and considerations we followed for a smooth transition. FIRST TIP: YOU MAY HAVE RESERVED WORDS IN YOUR TABLES Our first step was to delete "MVCC=true" from the JDBC URL parameters and add "MODE=LEGACY" to keep the compatibility. However, this was not enough as the following exception was raised. org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "CORRELATIONPROPERTYINFO" not found; SQL statement: insert into CorrelationPropertyInfo (correlationKey_keyId, name, value, OPTLOCK, propertyId) values (?, ?, ?, ?, ?) [42102-220] Diving in the logs, we found the culprit: Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "create table CorrelationPropertyInfo (propertyId bigint generated by default as identity, name varchar(255), [*]value varchar(255), OPTLOCK integer, correlationKey_keyId bigint, primary key (propertyId))"; expected "identifier"; SQL statement: So, the exception message states that the CorrelationPropertyInfo table cannot be created. The root cause is this table has a column named "value" and this is a reserved word for H2; therefore, it fails from version 200. In this case, H2 documentation mentions a workaround we can use: “ that can be used as a temporary workaround if your application uses them as unquoted identifiers.” For a quick replacement in all files, we used this “sed” command: find ./ -type f -exec sed -i 's/MVCC=[Tt][Rr][Uu][Ee]/MODE=LEGACY;NON_KEYWORDS=VALUE/g' {} \; SECOND TIP: HIBERNATE PERHAPS NEED TO BE UPGRADED TOO The next challenge was a little bit difficult to find out. We realized that database cleanup was not correctly done, and after activating logs we saw this exception in tests: h2 SchemaDropperImpl$DelayedDropActionImpl] ERROR HHH000478: Unsuccessful: drop table if exists This hibernate issue explains the details about it, and the disruptive in H2 that provokes it.   Therefore, we need to upgrade hibernate-core to at least or . NOTE: In case your code contains some of the StandardDialectResolver constants, notice that they in 5.4.0 as stated in . Then, you will have to replace: DialectResolver resolver = StandardDialectResolver.INSTANCE; by DialectResolver resolver = new StandardDialectResolver(); THIRD TIP: DROP-SOURCE STRATEGY SWITCHES TO “SCRIPT” WITH DROP ALL OBJECTS In our container tests, however, the previous tip was not easy to apply. Wildfly version 23 is not updated yet to the hibernate-core library version that we need: ./cargo/installs/wildfly-dist-23.0.0.Final/wildfly-23.0.0.Final/modules/system/layers/base/org/hibernate/main/hibernate-core-5.3.20.Final.jar So what to do in this case? Instead of relying on metadata for dropping the tables (default behavior), let’s make a trick and set that the schema shall be dropped based on a script. We need to add these two properties with their corresponding variables: &lt;property name="javax.persistence.schema-generation.drop-source" value="${org.jbpm.drop.source}" /&gt; &lt;property name="javax.persistence.schema-generation.drop-script-source" value="${org.jbpm.drop.script}" /&gt; Where default values are defined as: &lt;org.jbpm.drop.source&gt;script&lt;/org.jbpm.drop.source&gt; &lt;org.jbpm.drop.script&gt;${project.basedir}/src/test/resources/drop-tables.sql&lt;/org.jbpm.drop.script&gt; Notice that they can be overridden for keeping the same behavior for the rest of the databases and path must be absolute. And for H2, drop-tables.sql is not considering a CASCADE strategy but a single-shot action, avoiding the issue with the foreign keys: DROP ALL OBJECTS FOURTH TIP: CREATING DATABASE FROM TCP CONNECTION NEEDS “IFNOTEXISTS” The next error was for those databases created remotely from a TCP connection. Exception showed us that the database didn’t exist: Database … not found, either pre-create it or allow remote database creation (not recommended in secure environments) Older versions of H2 () allowed the database creation by default, introducing a security hole in your system. So, we need to set up the parameter "ifNotExists" explicitly when . Again, with the "sed" command, we can rapidly modify all files containing that pattern: find ./ -type f -exec sed -i 's/createTcpServer(new String\[0\])/createTcpServer(new String\[\]{"-ifNotExists"})/g' {} \; NOTE: Take into account this advice from : "Its combination with -tcpAllowOthers, -pgAllowOthers, or -webAllowOthers effectively creates a remote security hole in your system, if you use it, always guard your ports with a firewall or some other solution and use such combination of settings only in trusted networks.” CONCLUSION For many reasons (security and bug fixes, performance improvements, compatibility, new features, community support, and ecosystem compatibility) is very important to upgrade open-source libraries to the latest stable version. In this post, we have followed the process for stabilizing our tests after upgrading H2 from 1.4.197 (set two years ago) to the 2.2.220 version.  We will be delighted if any of the tips for H2 upgrade shared here prove helpful to the community. Happy upgraded testing! The post appeared first on .</content><dc:creator>Gonzalo Muñoz Fernández</dc:creator></entry><entry><title>Discover 3 advantages of Podman over Docker</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/03/3-advantages-docker-podman" /><author><name>Aine Keenan</name></author><id>2acf5595-4721-4ec9-9ad8-bc740ec57fe1</id><updated>2023-08-03T07:00:00Z</updated><published>2023-08-03T07:00:00Z</published><summary type="html">&lt;p&gt;Many think of Podman to be a replacement for Docker (if they have heard of Podman at all). But, this is not the case, as Podman is another option that provides better &lt;a href="https://developers.redhat.com/topics/security/"&gt;security&lt;/a&gt; and developer features. Podman is a cloud-native, daemonless tool that helps developers manage their &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;. Podman is all about security, but also minimizing the friction between your local development environment and production.&lt;/p&gt; &lt;p&gt;Podman uses a &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; approach, creating a network with many other cloud-native products, such as Buildah and Skopeo, to build and push containers. This makes Podman a lighter and faster application than Docker, allowing for customization and changes.&lt;/p&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;In this article, we will describe three advantages of Podman related to extensions and embedded tools integrated in the Podman Desktop, as well as the underlying technologies for the container engine.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;#1: Podman makes creating pods easy&lt;/h2&gt; &lt;p&gt;Creating pods, or podify, is a feature that allows you to combine running containers to create pods. It is as simple as selecting the containers you need, clicking the pod symbol, and customizing the pod. You can choose the name, the ports you would like exposed, and more (Figure 1). The ease of creating pods is unmatched by any other container engine, whether CLI or GUI.&lt;/p&gt; &lt;p&gt;After your pod is created, you will see it in the container and pod section. Developers have access to a variety of details about the pod, just like for any &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster. First, you will be greeted with a summary and logs. In the inspect section, you can access all low-level information about your pod, including the infrastructure container ID, all container information, network and namespace options, and more.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podify.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podify.png?itok=V37ZRKaX" width="600" height="419" alt="A screenshot of Podman Desktop for creating a pod." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Creating a pod using Podman Desktop.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;#2: Podman's Kubeify feature&lt;/h2&gt; &lt;p&gt;The most captivating feature of Podman Desktop is the Kubeify feature. Unlike other OCI container engines, there is zero friction moving from your container, pod, or volume to a Kubernetes pod. For any item, developers simply look under the Kube section, and they will be greeted with a Kubernetes manifest to make that item a Kubernetes pod (Figure 2). If you feel more comfortable with the CLI, the command &lt;code&gt;podman generate kube &lt;name of object&gt;&lt;/code&gt; will also work.&lt;/p&gt; &lt;p&gt;Additionally, Podman Desktop takes Kubeify a step further than just generating Kubernetes manifests. Extensions allow developers to connect their Podman Desktop, and therefore all of their manifests, containers, pods, etc, to a cluster of their choice. It comes with three suggested extensions developers can use for a local or remote cluster: the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, Kind, and &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;Red Hat OpenShift Local&lt;/a&gt;. Additionally, if developers have another local or remote cluster they prefer to use, they can input the image name and add it to their desktop. For more information about how to install Podman Desktop extensions, visit the &lt;a href="https://podman-desktop.io/docs/extensions/install"&gt;Podman Desktop site&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;After you have added your preferred local or remote Kubernetes cluster, select which service you would like to deploy and click &lt;strong&gt;Deploy to Kubernetes&lt;/strong&gt;. It is as simple as creating your container/pod, selecting your preferred cluster, and clicking deploy.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kube-ify.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kube-ify.png?itok=-fIavYkF" width="600" height="419" alt="The Podman Desktop auto-generated Kubernetes manifest for a pod." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Podman Desktop auto-generated Kubernetes manifest for a pod.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;#3: Better tools and extensions&lt;/h2&gt; &lt;p&gt;Podman offers better tools and extensions, such as pulling images, security, and auditing features.&lt;/p&gt; &lt;h3&gt;Pulling images&lt;/h3&gt; &lt;p&gt;Evidently, Podman and Docker have to allow developers to pull an image, since both products are container engines. You can do this using the terminal.&lt;/p&gt; &lt;p&gt;Unlike Docker, you can pull an image while in the Podman Desktop application. Docker will force you to head to the terminal. While the switch is annoying and can disrupt the workflow, the real kicker is that when using the Docker free plan, there are limited pulls a day unless you pay to upgrade.&lt;/p&gt; &lt;p&gt;With a team of over 500 employees, Docker is able to provide a breadth to their platform for which Podman seems to be aiming. To a developer, the first feature that stands out in Docker is the variety of extensions they offer, totaling over 60. With suggested offerings from MongoDB, Live Charts, Grafana, and more, it is eye-grabbing. Having these features improves developers' experience and removes the friction of adding them. However, if a developer wants to add their own extensions in Docker, it is very tedious, even downright impossible.&lt;/p&gt; &lt;p&gt;While Podman Desktop is lacking in pre-loaded extensions, they allow developers to add their own extensions by inputting the name of the image for the extensions. Any extensions that Docker has can be added to Podman Desktop.&lt;/p&gt; &lt;p&gt;Docker Desktop continues to flex with dev environments and the learning center. Dev environments, currently in Beta, work to have a code editor integrating the web platform. Additionally, they provide many walkthroughs and samples adding to the experience and growth of developers.&lt;/p&gt; &lt;h3&gt;Podman offers better security&lt;/h3&gt; &lt;p&gt;While Podman Desktop and Docker Desktop are the user interfaces, the underlying engines have significant differences as well.&lt;/p&gt; &lt;p&gt;Podman has opted for a rootless model, meaning the container system is, by default, run by a non-root user. Even if a user inside the container is the root user, once they are outside of the container, they have as much access as a non-root. Podman opts to run containers with user namespaces and SELinux containers, so if a container is attacked, the attacker only has access to files in the container. They cannot mess with the host or other containers due to incorrect permissions.&lt;/p&gt; &lt;p&gt;While Docker has offered support for a rootless daemon since version 19, by default the Docker daemon runs as root user. Thus, if it is compromised, the attacker has instant access to the entire system. The rootful Docker daemon is a background process on the host, so it needs full access to the system. Since containers are run through the daemon, a malicious container could control the host.&lt;/p&gt; &lt;h3&gt;Auditing features&lt;/h3&gt; &lt;p&gt;Podman has emphasized the importance of security, opting for a fork/execute model, while Docker has a client/server approach.&lt;/p&gt; &lt;p&gt;All Docker commands trigger the Docker client to send a request to the Docker daemon (the server). The Docker daemon is a child of the init system, causing all systemd, daemon, and container processes to share the same login. If a user ID for the init system is unset, all other recorded processes will have an unset user ID, allowing anonymous use.&lt;/p&gt; &lt;p&gt;Podman runs as a process and when given a command, forks itself, and has the new replica execute the new command. Multiple users can run Podman with each process forked and run separately.&lt;/p&gt; &lt;p&gt;Since Podman runs from a process model, all individual processes and containers are recorded. All processes stem from a certain user, meaning Podman’s logs will record a user ID, allowing for more correct and secure auditing.&lt;/p&gt; &lt;h2&gt;Switching to Podman is easy&lt;/h2&gt; &lt;p&gt;&lt;span&gt;Podman offers a variety of other developer tools that make building and pulling an image, playing Kubernetes YAML and pruning unused files a one-stop process. Podman and the desktop work together to ensure Podman is integrated into the Linux ecosystem.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Migrating from Docker to Podman has only gotten easier (as described in this &lt;a href="https://developers.redhat.com/blog/2020/11/19/transitioning-from-docker-to-podman"&gt;article&lt;/a&gt;). Podman Desktop has extensions that allow for migration from Docker and Docker compose to the Podman ecosystem. While Podman may be viewed as similar to Docker, it has many developer, security, and Linux-based features that contribute to its advantage. Consider making the leap from Docker to Podman. With the ease of migration, the leap isn’t far.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/03/3-advantages-docker-podman" title="Discover 3 advantages of Podman over Docker"&gt;Discover 3 advantages of Podman over Docker&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Aine Keenan</dc:creator><dc:date>2023-08-03T07:00:00Z</dc:date></entry><entry><title>A beginner's guide to Git version control</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/02/beginners-guide-git-version-control" /><author><name>Mohammadi Iram</name></author><id>f3a9ba14-0e05-43ab-973a-d74583864c6a</id><updated>2023-08-02T07:00:00Z</updated><published>2023-08-02T07:00:00Z</published><summary type="html">&lt;p&gt;Git is a widely used distributed version control system that allows software development teams to have multiple local copies of the project's source code that are independent of each other. Version control has come to be associated with Git—it is unquestionably the best version control program for new developers to start learning due to its popularity and wealth of resources related to its use. Read on for an overview of the basics.&lt;/p&gt; &lt;h2&gt;Git configuration: Linux&lt;/h2&gt; &lt;p&gt;Most Linux installations have Git, but to check, execute the following command in your terminal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git --version&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should get output that is similar to the following:&lt;/p&gt; &lt;pre&gt; git version 2.40.1&lt;/pre&gt; &lt;h2&gt;Git configuration: Windows&lt;/h2&gt; &lt;p&gt;Git searches the &lt;code&gt;$HOME&lt;/code&gt; directory for the &lt;code&gt;.gitconfig&lt;/code&gt; file on Windows systems.&lt;/p&gt; &lt;p&gt;We need to tell Git to keep track of our login and email when we use it, as shown in the code snippet below. This makes it possible for other code contributors to identify the change's author and our contact information in case of problems.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git config --global user.name "username" $ git config --global user.email "useremail"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use the following command if you need assistance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git help config&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will open a browser containing configuration commands. Essentially, the &lt;code&gt;help&lt;/code&gt; command gives a manual from the help page for the given command.&lt;/p&gt; &lt;p&gt;You can also use the same command in the following ways:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git config --help&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use the following command to view configurations:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git config -l&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Working with repositories&lt;/h2&gt; &lt;p&gt;A directory that Git will track is called a &lt;strong&gt;repository&lt;/strong&gt;,&lt;strong&gt; &lt;/strong&gt;or repo. The Git repository contains the whole of our project. Git will trace any change we make. We'll use the command below to create a test directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir test&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can then use the following command to enter the test directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd test&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Running &lt;code&gt;git init&lt;/code&gt; command inside the directory lets Git know that it is a Git repository:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git init  Empty Git repository created and initialised in /home/uname/test/.git&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now a Git repository exists in this directory. Showing the &lt;code&gt;.git&lt;/code&gt; file that Git generated inside the directory will be helpful; use the command as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ls -a&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; .git&lt;/pre&gt; &lt;p&gt;The directory is now a Git repository. In fact, deleting the &lt;code&gt;.git&lt;/code&gt; directory will uninitialize the repository (this can be very helpful when you're just starting out). Run the following command to make your directory a non-Git repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ rm.git -rf&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use this only if you truly want to start over because it will remove Git from the directory completely.&lt;/p&gt; &lt;p&gt;Let's create two files in the test directory with the names &lt;code&gt;file1&lt;/code&gt; and &lt;code&gt;file2&lt;/code&gt;. This will show how Git tracks files:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ touch file1.txt $ touch file2.txt&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Staging files: git add &lt;/h2&gt; &lt;p&gt;The principles of the staging environment and the commit are essential to Git. You can add, change, or remove files as you work. However, anytime you reach an important stage or complete a portion of the work, you should move the files to a staging environment. Files that have been &lt;strong&gt;staged&lt;/strong&gt; are ready to be committed to the repository you are working on. (We'll discuss commits in more detail in a later section.)&lt;/p&gt; &lt;p&gt;We can see what Git is tracking using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git status On branch main No commits yet Untracked files:   (use "git add &lt;file&gt;..." to include in what will be committed)         file1.txt         File2.txt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;git status&lt;/code&gt; displays a list of newly added or modified files and directories in the Git repository. In our example, &lt;code&gt;file1.txt&lt;/code&gt; and &lt;code&gt;file2.txt&lt;/code&gt; have been modified.&lt;/p&gt; &lt;p&gt;We must stage the files in order to instruct Git to keep track of changes. Let's use the &lt;code&gt;add&lt;/code&gt; command to stage &lt;code&gt;file1.txt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git add file1.txt $ git status On branch master No commits yet Changes to be committed:    (use "git rm --cached &lt;file&gt;..." to unstage)         new file: file1.txt Untracked files:   (use "git add &lt;file&gt;..." to include in what will be committed)         file2.txt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice that &lt;code&gt;file1.txt&lt;/code&gt; is listed under &lt;strong&gt;Changes to be committed&lt;/strong&gt; by Git. This indicates that Git is keeping note of any modifications made to this file in order to commit those modifications to the repository. Let's add &lt;code&gt;file2.txt&lt;/code&gt; now:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git add file2.txt $ git status ... Changes to be committed:   (use "git rm --cached &lt;file&gt;..." to unstage)         new file: file1.txt         new file: file2.txt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can instruct Git to monitor a file or directory using the &lt;code&gt;add&lt;/code&gt; command or to stop tracking a file or directory with the &lt;code&gt;unstaging&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;The git &lt;code&gt;rm&lt;/code&gt; command is used to remove individual files or a set of files using the filename, as shown below. The &lt;code&gt;git rm --cached&lt;/code&gt; command maintains a file in the working directory while removing it from the Git index. &lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git rm --cached file1.txt rm file1.txt $ git status … Untracked files: (   use "git add &lt;file&gt;..." to include in what will be committed)         file1.txt &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;file1.txt&lt;/code&gt; file is no longer being tracked.&lt;/p&gt; &lt;h2&gt;Keeping track of untracked files&lt;/h2&gt; &lt;p&gt;We frequently need a quick way to tell Git to add everything that is untracked to the staging area. We can do this by using &lt;code&gt;*&lt;/code&gt; or &lt;code&gt;.&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git add --all&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git add .&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git add *&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Stage all changes (new, changed, and deleted) files by using &lt;code&gt;--all&lt;/code&gt; rather than specific filenames.&lt;/p&gt; &lt;h2&gt;Commits&lt;/h2&gt; &lt;p&gt;Git uses &lt;strong&gt;commits&lt;/strong&gt; to make changes to files and directories permanent. In a sense, every commit represents a new version of our repository. Even while a commit can be seen to be a more permanent change, Git makes it simple to undo those changes, which is the strength of version control with Git.&lt;/p&gt; &lt;p&gt;For Git users, this alters the fundamental development model. Git developers have the option to build up commits in their local repo before making a change and committing it to the main repository. It accomplishes this by tracking the history of commits. Git's main purpose is to allow users to make commits. Everything we wanted to stage has already been done, so we can now make those modifications. &lt;code&gt;git commit&lt;/code&gt; is used to do this.&lt;/p&gt; &lt;p&gt;When using the &lt;code&gt;git commit&lt;/code&gt; command, we recommend always including two arguments: &lt;code&gt;-a&lt;/code&gt; and &lt;code&gt;-m&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Stage all modified files: git commit -a&lt;/h3&gt; &lt;p&gt;Add all untracked files to the staging area with the &lt;code&gt;-a&lt;/code&gt; or &lt;code&gt;--all&lt;/code&gt; option. Note that only previously added files and folders are added using this method. Use the &lt;code&gt;add&lt;/code&gt; command first if a file or directory has to be added that hasn't already been.&lt;/p&gt; &lt;h3&gt;Commit messages: git commit -m&lt;/h3&gt; &lt;p&gt;A commit message should always be included when making a commit. The &lt;code&gt;-m&lt;/code&gt; or &lt;code&gt;--message&lt;/code&gt; option is used for this. This message should be brief and descriptive, with just enough information included in the commit statement to summarize your actions since the last commit.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git commit -am "my first git commit"&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;View change history: git log&lt;/h3&gt; &lt;p&gt;To view the history of changes that have been committed to a Git repository, use the &lt;code&gt;git log&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git log&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In order to make our output easier to read and only to show the first seven characters of the commit ID, we'll also use the option &lt;code&gt;--oneline&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git log -oneline&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; cf0p490 (HEAD -&gt; main) my first git commit&lt;/pre&gt; &lt;p&gt;In this example, the commit ID's first seven characters are &lt;code&gt;cf0p490&lt;/code&gt;. There will be a lot of commits; thus each one needs to have its own identification. We can move the &lt;code&gt;HEAD&lt;/code&gt; pointer around as necessary.&lt;/p&gt; &lt;h2&gt;Publishing changes: git push&lt;/h2&gt; &lt;p&gt;Finally, the &lt;code&gt;git push&lt;/code&gt; command is used to upload content from a local repository to a remote repository. &lt;strong&gt;Pushing&lt;/strong&gt; refers to the process of transferring commits from your local repository to a remote repository.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git push&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git push origin branchname&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Where to learn more&lt;/h2&gt; &lt;p&gt;Explore more Git resources on Red Hat Developer for new and advanced users:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/cheat-sheets/git-cheat-sheet"&gt;Git cheat sheet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/20/git-workflows-best-practices-gitops-deployments"&gt;Git best practices: Workflows for GitOps deployments&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/02/protect-secrets-git-cleansmudge-filter"&gt;Protect secrets in Git with the clean/smudge filter&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2020/02/25/how-to-ignore-files-in-git-without-gitignore"&gt;How to ignore files in Git without .gitignore&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/02/beginners-guide-git-version-control" title="A beginner's guide to Git version control"&gt;A beginner's guide to Git version control&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mohammadi Iram</dc:creator><dc:date>2023-08-02T07:00:00Z</dc:date></entry></feed>
